# Load test nÂ°0, 24/02/2022

## Context 

We want to test the evolution and the impact of increasing the number of participant and conference in Jitsi Meet. In the same time, we would like to explore the different metrics that are fetched on the infrastructure and be able to estimate the values of those metrics.

---

## Description of the infrastucture

The Jitsi infrastucture we are working on is deployed on Kubernetes on Scaleway. It is based on the deployment available on the [jitsi-k8s repository of OpenFUN](https://github.com/openfun/jitsi-k8s/tree/59bdc9c799db3f0decedbb4b6f870f246091d7c8). More precisely, here are the specs of the JVB nodepool on our cluster:
- 1 server
- 4 CPU
- 16 GB of RAM
- 1 pod
- no HPA
- no resource limits on the JVB pod

---

## Approach

We deployed Jitsi-Meet-Torture instances in the cloud (on multiple instances on Scaleway) to apply a high load on the infrastructure. We can therefore legitimately assume that we emulate the perfect participants in our conferences that send and receive audio and video without any client-side limit.

We started with five conferences of ten participants each. Then we followed these steps:

| Time  | Number of conference added | Number of participant added | Expected number of conference |
| ----- | -------------------------- | --------------------------- | ----------------------------- |
| 15:12 | 5                          | 50                          | 5                             |
| 15:19 | 1                          | 10                          | 6                             |
| 15:23 | 1                          | 10                          | 7                             |
| 15:26 | 1                          | 10                          | 8                             |
| 15:29 | 1                          | 10                          | 9                             |
| 15:32 | 1                          | 10                          | 10                            |
| 15:37 | 1                          | 10                          | 11                            |
| 15:40 | 1                          | 10                          | 12                            |
| 15:43 | 1                          | 10                          | 13                            |
| 15:47 | 1                          | 10                          | 14                            |
| 15:51 | 1                          | 10                          | 15                            |
| 15:55 | 1                          | 10                          | 16                            |
| 15:59 | 1                          | 10                          | 17                            |
| 16:02 | 1                          | 10                          | 18                            |
| 16:07 | 1                          | 10                          | 19                            |
| 16:11 | 1                          | 10                          | 20                            |
| 16:15 | 1                          | 10                          | 21                            |
| 16:19 | 1                          | 10                          | 22                            |
| 16:23 | 1                          | 10                          | 23                            |
| 16:19 | 1                          | 10                          | 24                            |
| 16:19 | 1                          | 10                          | 25                            |
| 16:19 | 1                          | 10                          | 26                            |

---

## Results

All metrics were gathered with Prometheus and visualized with Grafana.

The process was to increase the number of participants and the number of conferences until we start seeing problems with video, audio or with the infrastructure itself. 
Looking at the Jitter Aggregate, there are two spikes. One at 15:17 and one at 15:45, the first one shows the beginning of the experience and the second one shows audio and video problems. 
Using the second spikes and our observation, we see that problems appear when we reach **120 participants**. 
Close to 120 participants, receive and transmit bandwidth starts dropping. Same goes for the rate of transmitted packets. 
We also observe that video and audio quality deteriorate for this number of participants. 


The number of participants have been tracked down to follow the evolution of metrics in terms of participants:

![Participants](resources/Participants.png)

Same goes for the number of conferences:

![Conferences](resources/Conferences.png)

### Graphs

| Metric                      | Source                                                                     |
| --------------------------- | -------------------------------------------------------------------------- |
| Bandwidth-Participants      | \[Bandwidth-Participants\](resources/Bandwidth-Participants.png)           |
| Transmit-Bandwidth          | \[Transmit-Bandwidth\](resources/Transmit-Bandwidth.png)                   |
| Conferences                 | \[Conferences\](resources/Conferences.png)                                 |
| CPU                         | \[CPU\](resources/CPU.png)                                                 |
| Jitter-Aggregate            | \[Jitter-Aggregate\](resources/Jitter-Aggregate.png)                       |
| Memory-Usage-WSS            | \[Memory-Usage-WSS\](resources/Memory-Usage-WSS.png)                       |
| Rate-of-Transmitted-Packets | \[Rate-of-Transmitted-Packets\](resources/Rate-of-Transmitted-Packets.png) |
| Participants                | \[Participants\](resources/Participants.png)                               |
| Rate-of-Received-Packets    | \[Rate-of-Received-Packets\](resources/Rate-of-Received-Packets.png)       |
| Receive-Bandwidth           | \[Receive-Bandwidth\](resources/Receive-Bandwidth.png)                     |
| RTT                         | \[RTT\](resources/RTT.png)                                                 |
| Sent-Received-Websockets    | \[Sent-Received-Websockets\](resources/Sent-Received-Websockets.png)       |

## Interpretation of results

We note that CPU and memory usage aren't responsible for this quality drop. They seem to have no influence on it. It takes more participants to make them reach their limits.
The only metric that seems relevant to interpret is bandwidth.
 
## Conclusion

According to the tests, bandwith seems to be the limiting factor. It sounds interesting to use this metric for autoscaling.
